{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f35cf1-755a-4612-918f-55026c4daf5c",
   "metadata": {},
   "source": [
    "## <u>Human-in-the-loop</u>\n",
    "\n",
    "Provide human feedback to a team.\n",
    "\n",
    "Two main ways to achieve this\n",
    "1. During .run/.run_stream - provide feedback through UserProxyAgent.\n",
    "2. Once the run terminates, provide feedback through .run/.run_stream\n",
    "\n",
    "#### <u>Off topic : code samples on integration with web and UI frameworks</u>\n",
    "\n",
    "1. <u>[Agentchat + FastAPI](https://github.com/microsoft/autogen/tree/main/python/samples/agentchat_fastapi)</u>\n",
    "2. <u>[Agentchat + ChainLit](https://github.com/microsoft/autogen/tree/main/python/samples/agentchat_chainlit)</u>\n",
    "3. <u>[Agentchat + StreamLit](https://github.com/microsoft/autogen/tree/main/python/samples/agentchat_streamlit)</u>\n",
    "\n",
    "### <u>Providing feedback during the run</u>\n",
    "\n",
    "As the name goes **UserProxyAgent** is a built-in agent that acts as a proxy for the user and provides feedback.\n",
    "\n",
    "Create an instance of UPA and include it in the team definition. The team will decide when to call UPA to ask for feedback.\n",
    "\n",
    "In **RoundRobinGroupChat**, UPA is called by order but in **SelectorGroupChat**, the selector prompt or selector function determines when the UPA is called.\n",
    "\n",
    "<img src=\"images/image_14.png\" width=600>\n",
    "\n",
    "Task & TaskResult arrows signify the normal input of task (run/run_stream) and the output TaskResult. The bold arrows show us the flow when the group chat decides to take an opinion from the user and then use that to continue finding the solution.\n",
    "\n",
    "<u>**Note:**</u> When the UPA is called, until a human replies, the state is suspended mid-execution. If the system crashes at that time, there is no way for the team to resume later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "235db540-cba7-4b84-aefb-19380eb4010b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d76c4ad-98c0-43a9-8deb-01984ecc350e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "Write a 4 line poem about Pune in Marathi.\n",
      "---------- TextMessage (assistant) ----------\n",
      "पुणे शहर, संस्कृतीची गळामिठी,  \n",
      "चहा-कॉफीचं, भेटींचं गाणं गाती,  \n",
      "सार्वजनिक उद्याने, सुखद सावली,  \n",
      "पुण्याची गोष्ट, हृदयात वसती.  \n",
      "\n",
      "TERMINATE\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your response:  do again\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user_promy) ----------\n",
      "do again\n",
      "---------- TextMessage (assistant) ----------\n",
      "पुणे म्हातारं, ज्ञानाचं घर,  \n",
      "पार्वती पर्वता, निसर्गात मजर,  \n",
      "छान गप्पा, चहा-भाजी,  \n",
      "पुण्याची माती, प्रेमभरी सावली.  \n",
      "\n",
      "TERMINATE\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your response:  APPROVE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user_promy) ----------\n",
      "APPROVE\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "assistant = AssistantAgent(\"assistant\", model_client=model_client)\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", input_func=input)\n",
    "\n",
    "termination = TextMentionTermination(\"APPROVE\")\n",
    "team = RoundRobinGroupChat([assistant, user_proxy], termination_condition=termination)\n",
    "\n",
    "await Console(team.run_stream(task=\"Write a 4 line poem about Pune in Marathi.\"))\n",
    "await model_client.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dead3c2b-4c23-4265-98cf-31a2a755ce8a",
   "metadata": {},
   "source": [
    "### <u>Providing feedback to the next run</u>\n",
    "\n",
    "In the last example, using UPA we were providing feedback within the same run.\n",
    "\n",
    "Now we will provide feedback within each run.\n",
    "\n",
    "This process runs best in a following format. A team completes a run. The state is saved in persistent storage. Then the team turns to the user input and then resume the run again when feedback arrives.\n",
    "\n",
    "There are two ways to implement this approach:\n",
    "1. Set max number : team always stops after # of runs.\n",
    "2. Use termination condition to give control back.\n",
    "\n",
    "<img src=\"images/image_15.png\" width=500>\n",
    "\n",
    "#### <u>Using max turns</u>\n",
    "\n",
    "In the below example, following pattern is shown. Chatbot pattern where the team terminates and gets user input after every 1 turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa990c7c-646b-4a20-830f-213722d8116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "Write 4 lines of information about a vacation destination in India.\n",
      "---------- TextMessage (assistant) ----------\n",
      "Rajasthan is a popular vacation destination in India, known for its rich history and vibrant culture. Visitors can explore majestic forts and palaces, such as the mesmerizing Amer Fort in Jaipur and the stunning City Palace. The Thar Desert offers unique experiences like camel safaris and traditional Rajasthani cuisine. Additionally, the annual Pushkar Camel Fair attracts tourists with its colorful festivities and cultural events.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your feedback (type 'exit' to leave):  now a different one\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "now a different one\n",
      "---------- TextMessage (assistant) ----------\n",
      "Goa is a renowned vacation destination in India, famous for its beautiful beaches and vibrant nightlife. With its charming Portuguese-influenced architecture, visitors can explore historic churches and lively markets. Goa offers a variety of water sports, including parasailing and scuba diving, making it perfect for adventure seekers. The laid-back atmosphere, combined with delicious seafood and beach shacks, makes it a haven for relaxation and enjoyment.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your feedback (type 'exit' to leave):  exit\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Create the agents.\n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "assistant = AssistantAgent(\"assistant\", model_client=model_client)\n",
    "\n",
    "# Create the team setting a maximum number of turns to 1.\n",
    "team = RoundRobinGroupChat([assistant], max_turns=1)\n",
    "\n",
    "task = \"Write 4 lines of information about a vacation destination in India.\"\n",
    "while True:\n",
    "    # Run the conversation and stream to the console.\n",
    "    stream = team.run_stream(task=task)\n",
    "    # Use asyncio.run(...) when running in a script.\n",
    "    await Console(stream)\n",
    "    # Get the user response.\n",
    "    task = input(\"Enter your feedback (type 'exit' to leave): \")\n",
    "    if task.lower().strip() == \"exit\":\n",
    "        break\n",
    "await model_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e88544-8322-476f-84f9-2183d8487326",
   "metadata": {},
   "source": [
    "#### <u>Using termination condition (handofftermination)</u>\n",
    "\n",
    "In this example we will look at handoff termination.\n",
    "\n",
    "Here is what happens in HandOff termination\n",
    "\n",
    "<img src=\"images/image_16.png\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2c3ba27-9eac-4710-8b3d-62ce02bfc7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "What is the weather in Pune right now?\n",
      "---------- ToolCallRequestEvent (lazy_assistant) ----------\n",
      "[FunctionCall(id='call_TVZNt4xWWv5PkN3rQNoYQbey', arguments='{}', name='transfer_to_user')]\n",
      "[Prompt tokens: 70, Completion tokens: 11]\n",
      "---------- ToolCallExecutionEvent (lazy_assistant) ----------\n",
      "[FunctionExecutionResult(content='Transfer to user.', name='transfer_to_user', call_id='call_TVZNt4xWWv5PkN3rQNoYQbey', is_error=False)]\n",
      "---------- HandoffMessage (lazy_assistant) ----------\n",
      "Transfer to user.\n",
      "---------- Summary ----------\n",
      "Number of messages: 4\n",
      "Finish reason: Handoff to user from lazy_assistant detected.\n",
      "Total prompt tokens: 70\n",
      "Total completion tokens: 11\n",
      "Duration: 1.86 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 10, 16, 18, 36, 11, 68824, tzinfo=datetime.timezone.utc), content='What is the weather in Pune right now?', type='TextMessage'), ToolCallRequestEvent(source='lazy_assistant', models_usage=RequestUsage(prompt_tokens=70, completion_tokens=11), metadata={}, created_at=datetime.datetime(2025, 10, 16, 18, 36, 12, 849323, tzinfo=datetime.timezone.utc), content=[FunctionCall(id='call_TVZNt4xWWv5PkN3rQNoYQbey', arguments='{}', name='transfer_to_user')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='lazy_assistant', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 10, 16, 18, 36, 12, 849323, tzinfo=datetime.timezone.utc), content=[FunctionExecutionResult(content='Transfer to user.', name='transfer_to_user', call_id='call_TVZNt4xWWv5PkN3rQNoYQbey', is_error=False)], type='ToolCallExecutionEvent'), HandoffMessage(source='lazy_assistant', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 10, 16, 18, 36, 12, 849323, tzinfo=datetime.timezone.utc), content='Transfer to user.', target='user', context=[], type='HandoffMessage')], stop_reason='Handoff to user from lazy_assistant detected.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.base import Handoff\n",
    "from autogen_agentchat.conditions import HandoffTermination, TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Create an OpenAI model client.\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "# Create a lazy assistant agent that always hands off to the user.\n",
    "lazy_agent = AssistantAgent(\n",
    "    \"lazy_assistant\",\n",
    "    model_client=model_client,\n",
    "    handoffs=[Handoff(target=\"user\", message=\"Transfer to user.\")],\n",
    "    system_message=\"If you cannot complete the task, transfer to user. Otherwise, when finished, respond with 'TERMINATE'.\",\n",
    ")\n",
    "\n",
    "# Define a termination condition that checks for handoff messages.\n",
    "handoff_termination = HandoffTermination(target=\"user\")\n",
    "# Define a termination condition that checks for a specific text mention.\n",
    "text_termination = TextMentionTermination(\"TERMINATE\")\n",
    "\n",
    "# Create a single-agent team with the lazy assistant and both termination conditions.\n",
    "lazy_agent_team = RoundRobinGroupChat([lazy_agent], termination_condition=handoff_termination | text_termination)\n",
    "\n",
    "# Run the team and stream to the console.\n",
    "task = \"What is the weather in Pune right now?\"\n",
    "await Console(lazy_agent_team.run_stream(task=task), output_stats=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44fa380-773a-44e9-a829-4a729487258f",
   "metadata": {},
   "source": [
    "<u>You see above that, since we have configured the team/agent using the HandOff and the HandOff condition was that \"if the agent does not know how to do a task, he will hand it off to the user\" so that is what he did.</u>\n",
    "\n",
    "Now, let us resume the team's run i.e. start a new run with a user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d45c7c6-4ed0-4bab-b1d4-5ed3cf55aa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "The weather in Pune is sunny.\n",
      "---------- TextMessage (lazy_assistant) ----------\n",
      "Thank you for the update! If you need any further information or assistance, feel free to ask.\n",
      "---------- ToolCallRequestEvent (lazy_assistant) ----------\n",
      "[FunctionCall(id='call_SZUV3iQBmshpM2YDT4JKUVar', arguments='{}', name='transfer_to_user')]\n",
      "---------- ToolCallExecutionEvent (lazy_assistant) ----------\n",
      "[FunctionExecutionResult(content='Transfer to user.', name='transfer_to_user', call_id='call_SZUV3iQBmshpM2YDT4JKUVar', is_error=False)]\n",
      "---------- HandoffMessage (lazy_assistant) ----------\n",
      "Transfer to user.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 10, 16, 18, 37, 58, 396754, tzinfo=datetime.timezone.utc), content='The weather in Pune is sunny.', type='TextMessage'), TextMessage(source='lazy_assistant', models_usage=RequestUsage(prompt_tokens=106, completion_tokens=21), metadata={}, created_at=datetime.datetime(2025, 10, 16, 18, 38, 0, 539149, tzinfo=datetime.timezone.utc), content='Thank you for the update! If you need any further information or assistance, feel free to ask.', type='TextMessage'), ToolCallRequestEvent(source='lazy_assistant', models_usage=RequestUsage(prompt_tokens=130, completion_tokens=11), metadata={}, created_at=datetime.datetime(2025, 10, 16, 18, 38, 3, 54090, tzinfo=datetime.timezone.utc), content=[FunctionCall(id='call_SZUV3iQBmshpM2YDT4JKUVar', arguments='{}', name='transfer_to_user')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='lazy_assistant', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 10, 16, 18, 38, 3, 54090, tzinfo=datetime.timezone.utc), content=[FunctionExecutionResult(content='Transfer to user.', name='transfer_to_user', call_id='call_SZUV3iQBmshpM2YDT4JKUVar', is_error=False)], type='ToolCallExecutionEvent'), HandoffMessage(source='lazy_assistant', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 10, 16, 18, 38, 3, 54090, tzinfo=datetime.timezone.utc), content='Transfer to user.', target='user', context=[], type='HandoffMessage')], stop_reason='Handoff to user from lazy_assistant detected.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await Console(lazy_agent_team.run_stream(task=\"The weather in Pune is sunny.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b757806-6f20-4234-8747-6e09b9da41c3",
   "metadata": {},
   "source": [
    "<u>Interestingly, it used the handoff again. So now it terminated the run and is waiting for the user.</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e9a9be-6db5-4f2c-94d5-56a657e225eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "terminate the session.\n",
      "---------- TextMessage (lazy_assistant) ----------\n",
      "TERMINATE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 10, 16, 18, 40, 0, 209910, tzinfo=datetime.timezone.utc), content='terminate the session.', type='TextMessage'), TextMessage(source='lazy_assistant', models_usage=RequestUsage(prompt_tokens=163, completion_tokens=4), metadata={}, created_at=datetime.datetime(2025, 10, 16, 18, 40, 1, 11451, tzinfo=datetime.timezone.utc), content='TERMINATE', type='TextMessage')], stop_reason=\"Text 'TERMINATE' mentioned\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await Console(lazy_agent_team.run_stream(task=\"terminate the session.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
